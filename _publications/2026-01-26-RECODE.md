---
title: "RECODE: A Benchmark for Research Code DEvelopment with Interactive Human Feedback"
collection: publications
permalink: /publication/2026-01-26-RECODE
date: 2025-01-26
venue: 'ICLR 2026'
paperurl: 'https://arxiv.org/abs/2510.06186'
authors: 'Chunyu Miao, Henry Peng Zou, Yangning Li, Yankai Chen, Yibo Wang, Fangxin Wang, <b>Yifan Li<b>, Wooseong Yang, Bowei He, Xinni Zhang, Dianzhi Yu, Hanchen Yang, Hoang H Nguyen, Yue Zhou, Jie Yang, Jizhou Guo, Wenzhe Fan, Chin-Yuan Yeh, Panpan Meng, Liancheng Fang, Jinhu Qi, Wei-Chieh Huang, Zhengyao Gu, Yuwei Han, Langzhou He, Yuyao Yang, Xue Liu, Irwin King, Philip S. Yu'

---

Large language models (LLMs) show the promise in supporting scientific research implementation, yet their ability to generate correct and executable code remains limited. Existing works largely adopt one-shot settings, ignoring the iterative and feedback-driven nature of realistic workflows of scientific research development. To address this gap, we present RECODE, a benchmark of 102 tasks from research papers and repositories that evaluates LLMs through multi-turn interactions with human feedback. It includes structured instructions, unit tests, and a five-level feedback hierarchy to reflect realistic researcherâ€“agent collaboration. We further present ReCodeAgent, a framework that integrates feedback into iterative code generation. Experimentswith leading LLMs, including GPT-5, Claude-Sonnet-4, DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer feedback, while also highlighting ongoing challenges in the generation of complex research code. RECODE establishes a foundation for developing adaptive, feedback-driven LLM agents in scientific research implementation.

<!-- [Download paper here](https://arxiv.org/abs/2509.23071)

Recommended citation: Muzhi Li, Jinhu Qi, Yihong Wu, Minghao Zhao, Liheng Ma, <b>Yifan Li</b>, Xinyu Wang, Yingxue Zhang, Ho-fung Leung, Irwin King. "From Evidence to Trajectory: Abductive Reasoning Path Synthesis for Training Retrieval-Augmented Generation Agents." <i>NeurIPS 2025 Workshop</i>. -->

